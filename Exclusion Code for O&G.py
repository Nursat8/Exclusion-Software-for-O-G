import re
import pandas as pd
import numpy as np
from io import BytesIO
import streamlit as st

# ðŸ”¹ Helper Functions ðŸ”¹
# ðŸ”¹ Drops duplicate labels when the same header is repeated twice in the Excel file. Keeps the first occurrence. It is needed because of multi sheet file with the same names in columns ðŸ”¹
def ensure_unique_columns(df):
    return df.loc[:, ~df.columns.duplicated()].copy()
    
# ðŸ”¹Takes a 2-level MultiIndex produced by header=[3,4] reads and collapses it into a single string per column ðŸ”¹    
def flatten_multilevel_columns(df):
    df.columns = [
        " ".join(str(l).strip() for l in col).strip()
        for col in df.columns
    ]
    return df

# ðŸ”¹Searches column headers using any of three modes: exact (â€œHydrocarbons Production %â€), partial (substring match), regex. Normalises spaces, case, and line-breaks before matching. Raises ValueError when required=True and nothing found ðŸ”¹ 
def find_column(df, patterns, how="partial", required=True):
    norm_map = {
        col: re.sub(r"\s+", " ", col.strip().lower().replace("\n", " "))
        for col in df.columns
    }
    pats = [
        re.sub(r"\s+", " ", p.strip().lower().replace("\n", " "))
        for p in patterns
    ]
    # exact
    for pat in pats:
        for col, norm in norm_map.items():
            if norm == pat:
                return col
    # partial
    if how == "partial":
        for col, norm in norm_map.items():
            for pat in pats:
                if pat in norm:
                    return col
    # regex
    if how == "regex":
        for pattern in patterns:
            for col in df.columns:
                if re.search(pattern, col, flags=re.IGNORECASE):
                    return col
    if required:
        raise ValueError(f"Could not find a required column among {patterns}\nAvailable: {list(df.columns)}")
    return None

# ðŸ”¹For every canonical name in rename_map, calls find_column() to locate the current spelling and renames it in-place. ðŸ”¹ 
def rename_columns(df, rename_map):
    for new, pats in rename_map.items():
        old = find_column(df, pats, how="partial", required=False)
        if old and old != new:
            df.rename(columns={old: new}, inplace=True)
    return df

# ðŸ”¹ Removes hard-space (\u00A0) characters. Strips any case-insensitive " Equity" suffix. Returns a copy so original df is untouched.ðŸ”¹ 
def remove_equity_from_bb_ticker(df):
    df = df.copy()
    if "BB Ticker" in df.columns:
        df["BB Ticker"] = (
            df["BB Ticker"]
              .astype(str)
              .str.replace(r"\u00A0", " ", regex=True)
              .str.replace(r"(?i)\s*Equity\s*", "", regex=True)
              .str.strip()
        )
    return df

# ðŸ”¹ðŸ”¹ðŸ”¹ Level 1 Exclusion ðŸ”¹ðŸ”¹ðŸ”¹
# ðŸ”¹ Opens "All Companies" sheet of uploaded file. Rows 4 and 5 (0-indexed) form a two-level column index. It is needed as column name located not in the first rows. Data clearing and remove "Equity" suffix; Ignores column with name parent companyðŸ”¹
def filter_companies_by_revenue(uploaded_file, sector_exclusions, total_thresholds):
    xls = pd.ExcelFile(uploaded_file)
    df = xls.parse("All Companies", header=[3,4])
    df.columns = [" ".join(map(str,c)).strip() for c in df.columns]
    df = df.loc[:, ~df.columns.str.lower().str.startswith("parent company")]
    df = remove_equity_from_bb_ticker(df)

    # ðŸ”¹ Canonical names for integrityðŸ”¹
    rename_map = {
        "Company": ["company name","company"],
        "BB Ticker": ["bb ticker"],
        "ISIN equity": ["isin equity"],
        "LEI": ["lei"],
        "Hydrocarbons Production (%)": ["hydrocarbons production"],
        "Fracking Revenue": ["fracking"],
        "Tar Sand Revenue": ["tar sands"],
        "Coalbed Methane Revenue": ["coalbed methane"],
        "Extra Heavy Oil Revenue": ["extra heavy oil"],
        "Ultra Deepwater Revenue": ["ultra deepwater"],
        "Arctic Revenue": ["arctic"],
        "Unconventional Production Revenue": ["unconventional production"]
    }
    df = rename_columns(df, rename_map)
   
    # ðŸ”¹ needed = list(rename_map.keys()) - build a list of every canonical column name. if column not in df.columns (excel) - if the column is missing in the DataFrame (in excel. df[c] = np.nan - create it and fill it entirely with NaN ðŸ”¹
    needed = list(rename_map.keys())
    for c in needed:
        if c not in df.columns:
            df[c] = np.nan

    # ðŸ”¹ no_data contains companies for which every revenue metric is missing or blank in the workbook. The result is that the main DataFrame df now includes only companies with at least one non-missing revenue value. ðŸ”¹
    revenue_cols = needed[4:]
    no_data = df[df[revenue_cols].isnull().all(axis=1)].copy()
    df = df.dropna(subset=revenue_cols, how="all")
 
    # ðŸ”¹ European comma â†’ US dot, percent sign removed, cast to float and corrupt values cleaning ðŸ”¹
    for c in revenue_cols:
        df[c] = (
            df[c]
              .astype(str)
              .str.replace("%","",regex=True)
              .str.replace(",","",regex=True)
        )
        df[c] = pd.to_numeric(df[c], errors="coerce").fillna(0)
   
    # ðŸ”¹ total_thresholds is a dictionary built from user inputs in the Streamlit sidebar. Filters out any missing columns to avoid a KeyError. ðŸ”¹
    # ðŸ”¹ "df[key] = df[secs].sum(axis=1) if secs else 0.0" adds up the values across the selected sector columns per row (i.e. per company). Later in the code, these new columns are used to apply custom sum exclusion rules ðŸ”¹
    for key,info in total_thresholds.items():
        secs = [s for s in info["sectors"] if s in df.columns]
        df[key] = df[secs].sum(axis=1) if secs else 0.0
        
    # ðŸ”¹ Build Level 1 exclusion reasons for output file ðŸ”¹
    # ðŸ”¹ Loops through each sector rule defined by the user. Each item has: flag: whether this exclusion is turned on. thr: threshold string (e.g., "10" meaning 10%). Also check whether value is in percentages or not ðŸ”¹
    reasons = []
    for _,r in df.iterrows():
        parts = []
        for sector,(flag,thr) in sector_exclusions.items():
            if flag and thr.strip():
                try:
                    if r[sector] > float(thr)/100:
                        parts.append(f"{sector} > {thr}%")
                except:
                    pass
    # ðŸ”¹ Each key is something like "Custom Total 1", "Custom Total 2". "reasons.append("; ".join(parts))" joins all reasons for the exclusion reasons rowðŸ”¹
        for key,info in total_thresholds.items():
            t = info.get("threshold","").strip()
            if t:
                try:
                    if r[key] > float(t)/100:
                        parts.append(f"{key} > {t}%")
                except:
                    pass
        reasons.append("; ".join(parts))
    df["Exclusion Reason"] = reasons
   
    # ðŸ”¹ Checks for empty and not empty exclusion reasons in excluded and retained companies sheets and creates copy for reasons ðŸ”¹
    excluded = df[df["Exclusion Reason"]!=""].copy()
    retained = df[df["Exclusion Reason"]==""].copy()

    # ðŸ”¹ For Custom Total 1 rename. If there is only 1 Custom TotalðŸ”¹
    if "Custom Total 1" in df.columns:
        for d in (excluded, retained, no_data):
            d.rename(columns={"Custom Total 1":"Custom Total Revenue"}, inplace=True)
  
    # ðŸ”¹ Fix any '.' company names. Makes sure the column is explicitly named "Company". ðŸ”¹
    # ðŸ”¹Removes any previous row index that might have been carried over from filtering or grouping. For any row where "Company" is now NaN, replace it with the corresponding entry from raw["Company"]ðŸ”¹
    # Fix any '.' company names
    raw = xls.parse("All Companies", header=[3,4]).iloc[:,[6]]
    raw = flatten_multilevel_columns(raw)
    raw = raw.loc[:, ~raw.columns.str.lower().str.startswith("parent company")]
    raw.columns = ["Company"]
    raw["Company"] = raw["Company"].fillna("").astype(str)
    for d in (excluded, retained, no_data):
        d.reset_index(drop=True, inplace=True)
        d["Company"] = d["Company"].replace(".", np.nan)
        d["Company"].fillna(raw["Company"], inplace=True)

    return excluded, retained, no_data

# ðŸ”¹ðŸ”¹ðŸ”¹ Level-2 â€” Upstream filter ðŸ”¹ðŸ”¹ðŸ”¹
# ðŸ”¹ Removes any columns that start with "Parent company" ðŸ”¹
def filter_upstream_companies(df):
    df = flatten_multilevel_columns(df)
    df = df.loc[:, ~df.columns.str.lower().str.startswith("parent company")]

    # ðŸ”¹ locate the columns (works with any spelling). Type of searching can be adjusted in "how =" ðŸ”¹
    comp_col      = find_column(df, ["company"], how="partial", required=True)
    res_col       = find_column(df, ["resources under development and field evaluation"],
                                how="partial", required=True)
    capex_avg_col = find_column(df, ["exploration capex 3-year average"],
                                how="partial", required=True)
    short_col     = find_column(df, ["short-term expansion â‰¥20 mmboe"],
                                how="partial", required=True)
    capex10_col   = find_column(df, ["exploration capex â‰¥10 musd"],
                                how="partial", required=True)

    # ðŸ”¹ rename to canonical spellings so every sheet matches ðŸ”¹
    df = df.rename(columns={
        comp_col     : "Company",
        res_col      : "Resources under Development and Field Evaluation",
        capex_avg_col: "Exploration CAPEX 3-year average",
        short_col    : "Short-Term Expansion â‰¥20 mmboe",
        capex10_col  : "Exploration CAPEX â‰¥10 MUSD",
    })

    # ðŸ”¹ numeric conversion ðŸ”¹
    num_cols = [
        "Resources under Development and Field Evaluation",
        "Exploration CAPEX 3-year average",
    ]
    for c in num_cols:
        df[c] = pd.to_numeric(
            df[c].astype(str)               # ðŸ”¹ Ensures even numeric or null cells are treated as strings.
                 .str.replace(",", "", regex=True)   # ðŸ”¹ Removes commas
                 .str.replace(r"[^\d.\-]", "", regex=True),   # ðŸ”¹ Removes everything except digits, decimal points, and minus signs.
            errors="coerce"
        ).fillna(0)


    # ðŸ”¹ Checks whether the company has any resources under development, invested any CAPEX over the past 3 years, short-term expansion exceeds 20 MMBOE, larger exploration projects with CAPEX â‰¥ $10 million, Exclude if any condition is true ðŸ”¹
    df["F2_Res"] = df["Resources under Development and Field Evaluation"] > 0
    df["F2_Avg"] = df["Exploration CAPEX 3-year average"] > 0
    df["F2_ST"]  = df["Short-Term Expansion â‰¥20 mmboe"].astype(str).str.lower().eq("yes")
    df["F2_10M"] = df["Exploration CAPEX â‰¥10 MUSD"].astype(str).str.lower().eq("yes")
    df["Excluded"] = df[["F2_Res","F2_Avg","F2_ST","F2_10M"]].any(axis=1)

    # ðŸ”¹ To populate the "Exclusion Reason" column in the DataFrame with text that explains which upstream condition(s) each company violated. Adds reasons if they are trueðŸ”¹
    df["Exclusion Reason"] = df.apply(
        lambda r: "; ".join(p for p in (
            "Resources under development and field evaluation > 0" if r["F2_Res"] else None,
            "3-yr CAPEX avg > 0" if r["F2_Avg"] else None,
            "Short-Term Expansion = Yes"   if r["F2_ST"]  else None,
            "CAPEX â‰¥10 MUSD = Yes"         if r["F2_10M"] else None,
        ) if p),
        axis=1
    )

    # ðŸ”¹ This final block of the filter_upstream_companies(df) function splits the dataset into two separate groups: excluded and retained companiesðŸ”¹
    exc = df[df["Excluded"]].copy()
    ret = df[~df["Excluded"]].copy()
    return exc[[
        "Company",
        "Resources under Development and Field Evaluation",
        "Exploration CAPEX 3-year average",
        "Short-Term Expansion â‰¥20 mmboe",
        "Exploration CAPEX â‰¥10 MUSD",
        "Exclusion Reason"
    ]], ret[[
        "Company",
        "Resources under Development and Field Evaluation",
        "Exploration CAPEX 3-year average",
        "Short-Term Expansion â‰¥20 mmboe",
        "Exploration CAPEX â‰¥10 MUSD",
        "Exclusion Reason"
    ]]

# ðŸ”¹ Excel Helpers ðŸ”¹
# ðŸ”¹ Creates output file columns. exc â†’ DataFrame of excluded companies (from Level 1 rules), ret â†’ DataFrame of retained companies, no_data â†’ DataFrame of companies that lacked enough input data to evaluate ðŸ”¹
def to_excel_l1(exc, ret, no_data):
    cols = [
        "Company","BB Ticker","ISIN equity","LEI",
        "Hydrocarbons Production (%)","Fracking Revenue","Tar Sand Revenue",
        "Coalbed Methane Revenue","Extra Heavy Oil Revenue","Ultra Deepwater Revenue",
        "Arctic Revenue","Unconventional Production Revenue","Exclusion Reason","Custom Total Revenue"
    ]
    exc     = remove_equity_from_bb_ticker(exc)
    ret     = remove_equity_from_bb_ticker(ret)
    no_data = remove_equity_from_bb_ticker(no_data)
    buf = BytesIO()
    with pd.ExcelWriter(buf, engine="xlsxwriter") as w:
        exc.reindex(columns=cols).to_excel(w, "Excluded Level 1", index=False)
        ret.reindex(columns=cols).to_excel(w, "Retained Level 1", index=False)
        no_data.reindex(columns=cols).to_excel(w, "L1 No Data", index=False)
    buf.seek(0)
    return buf

# ðŸ”¹ This function, to_excel_l2, generates a multi-sheet Excel report for Level 2 filtering, including results from all three filters: Level 1, midstream (All Companies), and upstream ðŸ”¹
def to_excel_l2(all_exc, exc1, exc2, ret1, ret2, exc_up, ret_up):
    cols = [
        # identity / Level-1 data
        "Company","BB Ticker","ISIN equity","LEI",
        "Hydrocarbons Production (%)","Fracking Revenue","Tar Sand Revenue",
        "Coalbed Methane Revenue","Extra Heavy Oil Revenue","Ultra Deepwater Revenue",
        "Arctic Revenue","Unconventional Production Revenue","Custom Total Revenue",
        # midstream
        "GOGEL Tab","Length of Pipelines under Development","Liquefaction Capacity (Export)",
        "Regasification Capacity (Import)","Total Capacity under Development",
        # upstream
        "Resources under Development and Field Evaluation","Exploration CAPEX 3-year average",
        "Short-Term Expansion â‰¥20 mmboe","Exploration CAPEX â‰¥10 MUSD",
        # â€¦and finally the reason string
        "Exclusion Reason"
    ]
    cols = list(dict.fromkeys(cols))   # keep order, drop accidental dups

    # remove duplicates while preserving order
    cols = list(dict.fromkeys(cols))
    for df in (all_exc, exc1, exc2, ret1, ret2, exc_up, ret_up):
        df.update(remove_equity_from_bb_ticker(df))

    buf = BytesIO()
    with pd.ExcelWriter(buf, engine="xlsxwriter") as w:
        all_exc.reindex(columns=cols).to_excel(w, "All Excluded Companies", index=False)
        exc1   .reindex(columns=cols).to_excel(w, "Excluded Level 1",      index=False)
        exc2   .reindex(columns=cols).to_excel(w, "Midstream Excluded",     index=False)
        ret1   .reindex(columns=cols).to_excel(w, "Retained Level 1",       index=False)
        ret2   .reindex(columns=cols).to_excel(w, "Midstream Retained",     index=False)
        exc_up .reindex(columns=cols).to_excel(w, "Upstream Excluded",      index=False)
        ret_up .reindex(columns=cols).to_excel(w, "Upstream Retained",      index=False)
    buf.seek(0)
    return buf


# ðŸ”¹ðŸ”¹ðŸ”¹ Streamlit App (UI)ðŸ”¹ðŸ”¹ðŸ”¹

def main():
    st.title("Level 1 & Level 2 Exclusion Filter for O&G")
    uploaded = st.file_uploader("Upload Excel file", type=["xlsx"])

    # ðŸ”¹ Level 1 sidebar ðŸ”¹
    st.sidebar.header("Level 1 Settings")
    sectors = [
        "Hydrocarbons Production (%)","Fracking Revenue","Tar Sand Revenue",
        "Coalbed Methane Revenue","Extra Heavy Oil Revenue","Ultra Deepwater Revenue",
        "Arctic Revenue","Unconventional Production Revenue",
    ]
    sector_excs = {}
    for s in sectors:
        chk = st.sidebar.checkbox(f"Exclude {s}")
        thr = ""
        if chk:
            thr = st.sidebar.text_input(f"{s} Threshold (%)")
        sector_excs[s] = (chk, thr)

    st.sidebar.header("Custom Total Thresholds")
    total_thresholds = {}
    n = st.sidebar.number_input("How many totals?", 1, 5, 1)
    for i in range(n):
        sels = st.sidebar.multiselect(f"Sectors for Total {i+1}", sectors, key=f"sel{i}")
        thr  = st.sidebar.text_input(f"Threshold {i+1} (%)", key=f"thr{i}")
        if sels and thr:
            total_thresholds[f"Custom Total {i+1}"] = {"sectors":sels,"threshold":thr}

    if st.sidebar.button("Run Level 1 Exclusion"):
        if not uploaded:
            st.warning("Please upload a file first.")
        else:
            exc1, ret1, no1 = filter_companies_by_revenue(uploaded, sector_excs, total_thresholds)
            st.success("Level 1 complete")
            st.download_button(
                "Download Level 1 Results",
                data=to_excel_l1(exc1, ret1, no1),
                file_name="O&G_Level1_Exclusion.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )

    st.markdown("---")
    st.header("Level 2 Exclusion")
    st.write("Applies All-Companies + Upstream filters, merges duplicates, and fills in all data.")

    if st.button("Run Level 2 Exclusion"):
        df_all = pd.read_excel(uploaded, "All Companies", header=[3, 4])
        df_all = ensure_unique_columns(df_all)      #  <-- after reading
        exc_all, ret_all = filter_all_companies(df_all)
       # ðŸ”¹ Upstream L2
        df_up = pd.read_excel(uploaded, "Upstream", header=[3, 4])
        df_up = ensure_unique_columns(df_up)        #  <-- after reading
        exc_up, ret_up = filter_upstream_companies(df_up)
        if not uploaded:
            st.warning("Please upload a file first.")
            return

        # ðŸ”¹ Rerun L1 to get full df_l1_all ðŸ”¹ 
        exc1, ret1, no1 = filter_companies_by_revenue(uploaded, sector_excs, total_thresholds)
        df_l1_all = pd.concat([exc1, ret1, no1], ignore_index=True)
        df_l1_all = ensure_unique_columns(df_l1_all)

        # ðŸ”¹ after you read the two Level-2 source sheets ðŸ”¹
        df_all = ensure_unique_columns(df_all)
        df_up  = ensure_unique_columns(df_up)

        # ðŸ”¹ after you build exc_all / ret_all, exc_up / ret_up ðŸ”¹
        exc_all = ensure_unique_columns(exc_all)
        exc_up  = ensure_unique_columns(exc_up)

        # ðŸ”¹ All-Companies L2 ðŸ”¹
        df_all = pd.read_excel(uploaded, "All Companies", header=[3,4])
        exc_all, ret_all = filter_all_companies(df_all)

        # ðŸ”¹ Upstream L2 ðŸ”¹
        df_up = pd.read_excel(uploaded, "Upstream", header=[3,4])
        exc_up, ret_up = filter_upstream_companies(df_up)

        # ðŸ”¹ Build All Excluded Companies union ðŸ”¹
        union = pd.concat([
            exc1[["Company"]],
            exc_all[["Company"]],
            exc_up[["Company"]]
        ]).drop_duplicates()

        # ðŸ”¹ Merge in Level 1 Reason ðŸ”¹ 
        df_l1_meta = df_l1_all[["Company","Exclusion Reason"]].rename(columns={"Exclusion Reason":"L1_Reason"})
        union = union.merge(df_l1_meta, on="Company", how="left")

        # ðŸ”¹ Merge in Level 2 All-Companies Reason ðŸ”¹
        union = union.merge(
            exc_all[["Company","Exclusion Reason"]].rename(columns={"Exclusion Reason":"L2_Reason_AC"}),
            on="Company", how="left"
        )
        # ðŸ”¹ Merge in Level 2 Upstream Reason ðŸ”¹
        union = union.merge(
            exc_up[["Company","Exclusion Reason"]].rename(columns={"Exclusion Reason":"L2_Reason_UP"}),
            on="Company", how="left"
        )

        # ðŸ”¹ Combine all three reasons into final Exclusion Reason ðŸ”¹
        union["Exclusion Reason"] = (
            union[["L1_Reason","L2_Reason_AC","L2_Reason_UP"]]
              .fillna("")
              .agg("; ".join, axis=1)
              .str.replace(r"(; )+", "; ")
              .str.strip("; ")
        )
        union.drop(columns=["L1_Reason","L2_Reason_AC","L2_Reason_UP"], inplace=True)
        meta_cols = df_l1_all.columns.difference(
            ["Company", "Exclusion Reason"]
        ).tolist()


        union = (
            union
                .merge(df_l1_all[["Company", *meta_cols]], on="Company", how="left")
                .merge(exc_all  .drop(columns=["Exclusion Reason"]), on="Company", how="left")
                .merge(exc_up   .drop(columns=["Exclusion Reason"]), on="Company", how="left")
        )
        union = union.merge(
            df_l1_all[["Company", "BB Ticker", "ISIN equity", "LEI"]],
            on="Company", how="left", suffixes=("", "_y")
        )
        union = union.drop(columns=[c for c in union.columns if c.endswith("_y")])

        # ðŸ”¹ Retained Level 2 ðŸ”¹
        all_names = set(df_l1_all["Company"])
        exc2_names = set(union["Company"])
        ret2 = pd.DataFrame({"Company":[c for c in all_names if c not in exc2_names]})
        ret2 = ret2.merge(df_l1_all, on="Company", how="left")

        # ðŸ”¹ Upstream full merge ðŸ”¹ 
        exc_up_full = (
            exc_up
            .merge(df_l1_all.drop(columns=["Exclusion Reason"]),
            on="Company", how="left")
        )
        ret_up_full = (
            ret_up
                .merge(df_l1_all.drop(columns=["Exclusion Reason"]),
                    on="Company", how="left")
        )

        
        buf = to_excel_l2(
            all_exc=union,
            exc1=exc1,
            exc2=(
                # Build Excluded Level 2 sheet properly with its own L2 reasons
                df_l1_all
                  .merge(exc_all[["Company","Exclusion Reason"]]
                           .rename(columns={"Exclusion Reason":"L2_Reason"}), 
                         on="Company", how="inner")
                  .assign(**{"Exclusion Reason": lambda d: d["L2_Reason"]})
                  .drop(columns=["L2_Reason"])
            ),
            ret1=ret1,
            ret2=ret2,
            exc_up=exc_up_full,
            ret_up=ret_up_full
        )
        st.success("Level 2 complete")
        st.download_button(
            "Download Combined Level 1 & 2 Results",
            data=buf,
            file_name="O&G_Level1_Level2_Exclusion.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )


# ðŸ”¹ Level-2 â€” Midstream / â€œAll-Companiesâ€ filter ðŸ”¹
def filter_all_companies(df: pd.DataFrame):
    """
    ðŸ”¹ Implements Level-2 â€˜mid-streamâ€™ screen on the **All Companies** sheet.
    Returns (excluded_df, retained_df) with the canonical columns
    and an â€˜Exclusion Reasonâ€™ column. ðŸ”¹
    """
    # 1. tidy columns ---------------------------------------------------------
    df = flatten_multilevel_columns(df)
    df = df.loc[:, ~df.columns.str.lower().str.startswith("parent company")]         
    df = df.iloc[1:].reset_index(drop=True)          # ðŸ”¹ Drops the first data row (df.iloc[1:]), which may contain merged header content or notes from Excel. Resets the index afterward so the rows are renumbered properly. ðŸ”¹
    df = ensure_unique_columns(df)

    # ðŸ”¹ 2. rename the few columns we care about ðŸ”¹
    rename_map = {
        "Company": ["company"],
        "GOGEL Tab": ["gogel tab"],
        "BB Ticker": ["bb ticker"],
        "ISIN equity": ["isin equity"],
        "LEI": ["lei"],
        "Length of Pipelines under Development": ["length of pipelines"],
        "Liquefaction Capacity (Export)":        ["liquefaction capacity"],
        "Regasification Capacity (Import)":      ["regasification capacity"],
        "Total Capacity under Development":      ["total capacity under development"],
    }
    df = rename_columns(df, rename_map)

    # ðŸ”¹ 3. make sure every canonical column exists ðŸ”¹
    needed = list(rename_map.keys())
    for c in needed:
        if c not in df.columns:
            df[c] = np.nan

    # ðŸ”¹ 4. numeric conversion for the four capacity columns ðŸ”¹
    for c in needed[5:]:
        df[c] = pd.to_numeric(
            df[c].astype(str).str.replace(",", "", regex=True),
            errors="coerce"
        ).fillna(0)

    # ðŸ”¹ 5. flag & reason. For midstream exclusion ðŸ”¹
    df["Midstream_Flag"] = (
        (df["Length of Pipelines under Development"] > 0) |
        (df["Liquefaction Capacity (Export)"]        > 0) |
        (df["Regasification Capacity (Import)"]      > 0) |
        (df["Total Capacity under Development"]      > 0)
    )
    df["Excluded"] = df["Midstream_Flag"]
    df["Exclusion Reason"] = np.where(
        df["Midstream_Flag"],
        "Midstream Expansion > 0",
        ""
    )

    excluded = df[df["Excluded"]].copy()
    retained = df[~df["Excluded"]].copy()
    return excluded, retained



if __name__ == "__main__":
    main()
